# -*- coding: utf-8 -*-
"""Copy of clase_metpot_alu_20242C.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-65m5Yi4RYj4mRkQyTu-EwcQtas4nbKv
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

"""# Cálculo de autovectores: Método de la potencia

## Ejercicio 1: Metodo de la potencia

Implementar el método de la potencia considerando algún criterio de parada
"""

import numpy as np

def power_iteration(A, niter=10_000, eps=1e-6):
    """
    Calcula el autovector al autovalor asociado de valor máximo

    Devuelve (a, v) con a autovalor, y v autovector de A

    Arguments:
    ----------

    A: np.array
        Matriz de la cual quiero calcular el autovector y autovalor

    niter: int (> 0)
        Cantidad de iteraciones

    eps: Epsilon
        Tolerancia utilizada en el criterio de parada
    """

    a = 1
    v = -np.ones(A.shape[0])
    for i in range(niter):
      v = (A @ v) / np.linalg.norm(A @ v)
      if (all(np.isclose(A @ v, a * v, atol=0.001))):
        return a, v, i

    a = (v.T @ A @ v) / (v.T @ v)


    return a, v, niter

"""Verifiquemos la implementación un ejemplo conocido:

$$
A = Q^T \begin{pmatrix}
d_1    &0     &0      &0      &\\
0      &d_2   &0      &0      &\\
\vdots &\vdots&\ddots &\vdots &\\
0      &0     &0      &d_n    & \\
\end{pmatrix} Q
$$

con $Q = I - 2 v v^T$, $||v||_2=1$
 la matriz de reflexión que sabemos que es ortogonal

Probemos calcular con el método de la potencia el autovector y autovalor dominante.
"""

#@title
import numpy as np

D = np.diag([5.0, 4.0, 3.0, 2.0, 1.0])

v = np.ones((D.shape[0], 1))

v = v / np.linalg.norm(v)

# Matriz de Householder
B = np.eye(D.shape[0]) - 2 * (v @ v.T)

# Matriz a diagonalizar, recordar B es simétrica y ortogonal
M = B @ D @ B.T

l, v, i = power_iteration(M,niter=10_000, eps=1e-20)
assert(np.isclose(l, 5))
print(v)
print(B[:,0])

assert(np.allclose(v,B[:,0])) # ¿esto debería pasar?

"""# Ejercicio 2: Metodo de la potencia + Deflación

Implementar método de la potencia + deflación (de Hotelling)
"""

def eigen(A, num=2, niter=10000, eps=1e-6):
    """
    Calculamos num autovalores y autovectores usando método de la potencia+deflación
    """
    A = A.copy()
    eigenvalues = []
    eigenvectors = np.zeros((A.shape[0], num))
    max_it = 0
    for i in range(num):
        l, v, it = power_iteration(A, niter, eps)
        max_it = max(it, max_it)
        eigenvalues.append(l)
        eigenvectors[:, i] = v
        A = A - l * np.outer(v,v)

    return np.array(eigenvalues), eigenvectors, max_it

"""## Casos de prueba

Matriz Diagonal
"""

D = np.diag(range(10, 0, -1))
# print(D)

#%%time prender si se quiere medir el tiempo
l, v = eigen(D,10,niter=10_000)
print(l, v)
assert(np.allclose(np.diag(l), D))

"""Otra matriz de Householder"""

N = 10
D = np.diag(range(N, 0, -1))

v = np.ones((D.shape[0], 1))
v = v / np.linalg.norm(v)

# Matriz de Householder
B = np.eye(D.shape[0]) - 2 * (v @ v.T)

M = B @ D @ B.T

# Para todos los del ejemplo anterior de householder.
l, v = eigen(M, N, niter=10_000,eps=1e-6)
print(l,v)
assert(np.allclose(np.diag(l), D)) # Completar

"""Matrices $A.A^T$ y $A^TA$"""

A = D
AT = A @ A.T
TA = A.T @ A
wta, VTA = eigen(AT, num=3, niter=20000, eps=1e-24)
wat, VAT = eigen(TA, num=3, niter=20000, eps=1e-24)

# Mismos autovalores
assert(np.allclose(wat,wta))

# ¿Mismos autovectores? No, pero están relacionados los de AT y los de TA
assert(np.allclose(VTA, VAT)) # Completar

"""# Ejercicio 3: Velocidad de convergencia

## Ejercicio 3.1:

Graficar, para la matriz M definida abajo, los valores de la sucesión {$\lambda\}^k$ del autovalor dominante. Considerar, si fuera necesario, extender el método implementado anteriormente para obtener la suecesión.
"""

D = np.diag([5.0, 4.0, 3.0, 2.0, 1.0])

v = np.ones((D.shape[0], 1))

v = v / np.linalg.norm(v)

# Matriz de Householder
B = np.eye(D.shape[0]) - 2 * (v @ v.T)

# Matriz a diagonalizar
M = B.T @ D @ B

power_iteration(M,...)

lambdas = ... # COMPLETAR
plt.plot(np.arange(len(lambdas),dtype="int"), lambdas)
plt.xlabel("k")
plt.ylabel("$\lambda_k$")
#plt.yscale('log')

"""## Ejercicio 3.2
Analizar la velocidad de convergencia para la sucesión {$\lambda_k\}_{k}$ del autovalor dominante $\lambda_1 = 5$.

Verificar que, en caso de tomar cociente de Rayleigh, se obtiene convergencia cuadrática:

$$ \{\lambda_k\}_{k} \in O((\frac{\lambda_2}{\lambda_1})^{2k})$$

Considerar, si fuera necesario, extender el método implementado anteriormente para obtener los errores $|\lambda_{k} - \lambda^*|$


---

**Recordemos que**:

Sea ${x_k}$ k ∈ N una sucesión tal que:

$$lim_{k→∞} x_k = x^*$$

Decimos que ${x_k}$, k ∈ N, tiene orden de convergencia $p$ si:

$$lim_{k→∞} \frac{|x_{k+1} − x^∗|}{ (|x_k − x^∗ |)^p} =  c > 0$$



Existen otras formas de definir la noción de velocidad de convergencia como la siguiente:



Sea ${\alpha_n}$ convergente a $\alpha$. Sea ${\beta_n}$ convergente a 0. Decimos que ${\alpha_n}$ tiene orden de convergencia $O(\beta_n)$ (es decir, ue $\alpha_n$ converge tan rápido como $\beta_{n}$) si existe c > 0 tal que:

$$|\alpha_n − \alpha| ≤ c\beta_n$$ para todo
n suficientemente grande.

---



"""

#
# COMPLETAR
#

"""## Ejercicio 3.3

Analizar la velocidad de convergencia de la sucesión {$v_k\}_{k}$ al autovector dominante.


Verificar que en este caso se da convergencia lineal:

$$ \{v_k\}_{k} \in O((\frac{\lambda_2}{\lambda_1})^{k})$$



**Imporante: ¿Contra que autovector solución comparamos?**

* En algunos casos podemos no conocer el vector en cuestión.
* Además para el caso de autovalores repetidos, no sabemos exactamente a que combinación del autoespacio cae el vector de la sucesión.

*Obs: Para el caso particular de la matriz M, ya vimos que la convergencia se daba a -v1 (v1 primera columna de B)*
"""

#
# COMPLETAR
#

"""Otra forma, la que se pide en el tp, mira que tan "buena" es la aproximación obtenida considerando:

$$ ||Av -\lambda v||$$

Observemos como evoluciona lo anterior para la matriz M en el calculo de su autovector dominante

$$ ||Mv_k -\lambda_k v_k||$$

"""

#
# COMPLETAR
#

"""# Ejercicio 4

Considerar matrices aleatorias construidas con "el truco de Householder" con los auotvalores dados por $[5, 5-e, 4, 4-e, 1]$ siendo $e = 0.01$.

Reportar y analizar la cantidad de iteraciones obtenido para cada autovector habilitando el método de corte (ajsutando niter y eps de manera que el allclose pase).  

Recordar mostrar alguna noción de centralidad y dispersión para cada grupo de mediciones.
"""

reps = range(50)
e = 0.01
niters = []

for r in reps:
  D = np.diag([5, 5-e, 4, 4-e, 1]).astype(np.float64)

  v = 4*np.random.randn(D.shape[0], 1)
  v = v / np.linalg.norm(v)

  # Matriz de Householder
  B = np.eye(D.shape[0]) - 2 * (v @ v.T)

  # Matriz a diagonalizar
  M = B.T @ D @ B

  l,V,it = eigen(M,4,niter=10_000,eps=e)
  for i in range(len(l)-1):
    assert(np.allclose(M@V[:,i], l[i] * V [:,i], atol=1e-6))

  niters.append(it)


# Plot niters
plt.boxplot(niters)
plt.show()

"""# Ejercicio 5:


Repetir el ejercicio anterior pero variando $e$ en un rango logaritmico de $10^{-5}$ hasta $10^0$.

Plotear dos gráficos:

* El error en función del epsilon. Mostrar una curva por autovector.

* La cantidad de iteraciones en función del epsilon. Mostrar una curva por autovector.

Recordar reportar los resultados con alguna medida de centralidad y dispersión (pueden usar plt.errorbar).

"""

